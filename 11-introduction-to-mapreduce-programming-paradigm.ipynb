{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Introduction to Hadoop MapReduce </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reality of working with Big Data\n",
    "\n",
    "- Hundreds or thousands of machines to support big data\n",
    "    - Distribute data for storage (HDFS)\n",
    "    - Parallelize data computation (Hadoop MapReduce)\n",
    "    - Handle failure (HDFS and Hadoop MapReduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce\n",
    "\n",
    "** What is “map”? **\n",
    "A function/procedure that is applied to every individual elements of a collection/list/array/…\n",
    "```\n",
    "int square(x) { return x*x;}\n",
    "map square [1,2,3,4] -> [1,4,9,16]\n",
    "```\n",
    "** What is “reduce”? **\n",
    "A function/procedure that performs an operation on a list. This operation will “fold/reduce” this list into a single value (or a smaller subset)\n",
    "```\n",
    "reduce ([1,2,3,4]) using sum -> 10\n",
    "reduce ([1,2,3,4]) using multiply -> 24\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation of MapReduce Programming Paradigm in Hadoop MapReduce\n",
    "\n",
    "**Programmers implement:**\n",
    "\n",
    "- Map function: Take in the input data and return a key,value pair\n",
    "- Reduce function: Receive the key,value pairs from the mapper and provide a final output as a reduction operation on the pairs\n",
    "- Optional functions:\n",
    "    - Partition function: determines the distribution of mappers’ key,value pairs to the reducers\n",
    "    - Combine functions: initial reduction on the mappers to reduce network traffics\n",
    "    \n",
    "**The MapReduce Framework handles everything else**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WordCount: The *Hello, World* of Big Data\n",
    "\n",
    "- Count how many unique words there are in a file/multiple files\n",
    "- Standard parallel programming approach:\n",
    "    - Count number of files\n",
    "    - Set number of processes\n",
    "    - Possibly setting up dynamic workload assignment\n",
    "    - A lot of data transfer\n",
    "    - Significant coding effort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce WordCount Example\n",
    "\n",
    "<img src=\"pictures/11/wordcount01.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce WordCount Example\n",
    "\n",
    "<img src=\"pictures/11/wordcount02.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce PageRank Example 1\n",
    "\n",
    "<img src=\"pictures/11/pagerank01.png\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce PageRank Example 2\n",
    "\n",
    "<img src=\"pictures/11/pagerank02.png\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is \"everything else\"?\n",
    "\n",
    "- Scheduling\n",
    "- Data distribution\n",
    "- Synchronization\n",
    "- Error and Fault Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The cost of \"everything else\"?\n",
    "\n",
    "- All algorithms must be expressed as a combination of mapping, reducing, combining, and partitioning functions \n",
    "- No control over execution placement of mappers and reducers\n",
    "- No control over life cycle of individual mappers and reducers\n",
    "- Very limited information about which mapper handles which data block\n",
    "- Very limited information about which reducer handles which intermediate key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional challenge\n",
    "\n",
    "** Large scale debugging on big data programming is difficult\n",
    "\n",
    "- Functional errors are difficult to follow at large scale\n",
    "- Data-dependent errors are even more difficult to catch and fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of MapReduce\n",
    "\n",
    "- Text tokenization, indexing, and search\n",
    "    - Web access log stats\n",
    "    - Inverted index construction\n",
    "    - Term-vector per host\n",
    "    - Distributed grep/sort\n",
    "- Graph creation\n",
    "    - Web link-graph reversal (Google’s PageRank)\n",
    "- Data Mining and machine learning\n",
    "    - Document clustering\t\n",
    "    - Machine learning\n",
    "    - Statistical machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Working with Hadoop MapReduce on CloudLab </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Profile Template\n",
    "\n",
    "https://github.com/linhbngo/example-cloudlab\n",
    "\n",
    "#### Today's Experiment\n",
    "\n",
    "https://www.cloudlab.us/status.php?uuid=78182751-ddf7-11e8-b339-90e2ba22fee4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- From a terminal, SSH to the namenode in the above experiment. \n",
    "- If you do not have your SSH key setup, use a shell to login to the above experiment, then setup a password for your account using\n",
    "\n",
    "```\n",
    "$ sudo passwd <your_user_name>\n",
    "```\n",
    "\n",
    "- From a terminal, SSH to the namenode using your login name and password\n",
    "- *This is necessary for the easy of copying codes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Run the followings:\n",
    "\n",
    "```\n",
    "$ hdfs dfs -ls .\n",
    "$ hdfs dfs -mkdir intro-to-hadoop\n",
    "$ hdfs dfs -ls .\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
